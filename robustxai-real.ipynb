{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":37484,"sourceType":"datasetVersion","datasetId":29414}],"dockerImageVersionId":30775,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"AttributeError# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"adde8ac2-1558-423e-a319-07ce8c17d77a","_cell_guid":"05ec5838-0604-4bea-8cd0-88a741b77830","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-10-03T04:33:40.093126Z","iopub.execute_input":"2024-10-03T04:33:40.093448Z","iopub.status.idle":"2024-10-03T04:33:41.130268Z","shell.execute_reply.started":"2024-10-03T04:33:40.093414Z","shell.execute_reply":"2024-10-03T04:33:41.129325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install captum==0.5.0\n# !pip install h5py==3.7.0\n# !pip install imbalanced-learn==0.9.1\n# !pip install joblib==1.2.0\n# !pip install matplotlib-base==3.5.3\n# !pip install networkx==2.8.4\n# !pip install numpy==1.23.4\n# !pip install pandas==1.5.1\n# !pip install pyg==2.2.0\n# !pip install torch==1.13.0\n# !pip install scikit-learn==1.1.3\n# !pip install seaborn==0.12.1\n# !pip install torchaudio==0.13.0\n# !pip install torchvision==0.14.0\n# !pip install tqdm==4.64.1\n# !pip install build==0.10.0\n# !pip install docopt==0.6.2\n# !pip install kaggle==1.5.12\n# !pip install pip-tools==6.12.3\n# !pip install pipreqs==0.4.11\n# !pip install pyproject-hooks==1.0.0\n# !pip install python-slugify==7.0.0\n# !pip install text-unidecode==1.3\n# !pip install yarg==0.1.9\n# !pip install pytorch-lightning==2.0.2\n# !pip install e2cnn==0.2.3\n# !pip install wandb==0.15.0\n# !pip install nltk==3.8.1","metadata":{"_uuid":"495eef4c-45ff-449f-adbe-16ae9f0b3cb2","_cell_guid":"c98ca857-03db-41fc-859d-61e813ca6465","collapsed":false,"execution":{"iopub.status.busy":"2024-10-02T11:46:03.958609Z","iopub.execute_input":"2024-10-02T11:46:03.959933Z","iopub.status.idle":"2024-10-02T11:56:00.119837Z","shell.execute_reply.started":"2024-10-02T11:46:03.959876Z","shell.execute_reply":"2024-10-02T11:56:00.116312Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/JonathanCrabbe/RobustXAI.git","metadata":{"_uuid":"8f7a2662-22e3-47dc-9287-aa14b12fce90","_cell_guid":"478898b3-02f6-4c89-bb99-5a511aa1a2bf","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-10-03T11:00:28.410801Z","iopub.execute_input":"2024-10-03T11:00:28.411401Z","iopub.status.idle":"2024-10-03T11:00:29.966098Z","shell.execute_reply.started":"2024-10-03T11:00:28.411360Z","shell.execute_reply":"2024-10-03T11:00:29.964987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install torch_geometric\n!pip install e2cnn==0.2.3\n!pip install captum==0.5.0","metadata":{"_uuid":"6ebd3761-3bfe-4caf-871a-00b344935b90","_cell_guid":"c78406c8-b8ba-4a47-a0f5-266e5b323949","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-10-03T11:00:31.111052Z","iopub.execute_input":"2024-10-03T11:00:31.111439Z","iopub.status.idle":"2024-10-03T11:01:09.219450Z","shell.execute_reply.started":"2024-10-03T11:00:31.111400Z","shell.execute_reply":"2024-10-03T11:01:09.218478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %cd /opt/conda/envs/robustxai","metadata":{"_uuid":"d1917c60-d967-49a1-9e1e-e83d03b28067","_cell_guid":"4c4c3cb6-8ddd-44fb-8c62-9acf4a588b4e","collapsed":false,"execution":{"iopub.status.busy":"2024-10-02T12:00:30.347962Z","iopub.execute_input":"2024-10-02T12:00:30.350169Z","iopub.status.idle":"2024-10-02T12:00:30.360592Z","shell.execute_reply.started":"2024-10-02T12:00:30.350082Z","shell.execute_reply":"2024-10-02T12:00:30.358613Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %cd /kaggle/working/RobustXAI","metadata":{"_uuid":"341b0e68-8686-4ea5-96c5-4acea5331269","_cell_guid":"ee56272a-57ce-467e-8fe4-3b46acb4f322","collapsed":false,"execution":{"iopub.status.busy":"2024-10-02T12:01:50.848493Z","iopub.execute_input":"2024-10-02T12:01:50.849096Z","iopub.status.idle":"2024-10-02T12:01:50.859979Z","shell.execute_reply.started":"2024-10-02T12:01:50.849043Z","shell.execute_reply":"2024-10-02T12:01:50.858317Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pwd","metadata":{"_uuid":"06023c57-28f9-4609-8677-9fbc82df208f","_cell_guid":"9657e015-4bcd-48b8-b529-139dd2dcaea5","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-10-03T06:23:28.268221Z","iopub.execute_input":"2024-10-03T06:23:28.268662Z","iopub.status.idle":"2024-10-03T06:23:29.252559Z","shell.execute_reply.started":"2024-10-03T06:23:28.268607Z","shell.execute_reply":"2024-10-03T06:23:29.251564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !mv /kaggle/working/RobustXAI/* ..\n# # !mv /kaggle/working/RobustXAI/.[!.]* .","metadata":{"_uuid":"ff1e5c33-6855-4f7d-8162-edc95a5265f6","_cell_guid":"99d165a9-fe26-4c41-ad6c-2d0b6ce9e9f2","collapsed":false,"execution":{"iopub.status.busy":"2024-10-02T12:54:53.118971Z","iopub.execute_input":"2024-10-02T12:54:53.119992Z","iopub.status.idle":"2024-10-02T12:54:54.168080Z","shell.execute_reply.started":"2024-10-02T12:54:53.119941Z","shell.execute_reply":"2024-10-02T12:54:54.166730Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mv /kaggle/working/RobustXAI/* /kaggle/working/","metadata":{"_uuid":"3ea3b87d-4dbb-4c67-af6b-d8dff67d556f","_cell_guid":"330ad317-c95d-4a90-82a8-a751408ec0e1","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-10-03T11:01:09.221439Z","iopub.execute_input":"2024-10-03T11:01:09.221789Z","iopub.status.idle":"2024-10-03T11:01:10.233675Z","shell.execute_reply.started":"2024-10-03T11:01:09.221754Z","shell.execute_reply":"2024-10-03T11:01:10.232394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!rm -d /kaggle/working/RobustXAI","metadata":{"_uuid":"8e77c10e-c16d-4908-a7fb-0c42f6aa8d60","_cell_guid":"2ffb80f0-4701-48c5-bf43-b2dd8e9a9221","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-10-03T08:55:19.830109Z","iopub.execute_input":"2024-10-03T08:55:19.830973Z","iopub.status.idle":"2024-10-03T08:55:20.875301Z","shell.execute_reply.started":"2024-10-03T08:55:19.830928Z","shell.execute_reply":"2024-10-03T08:55:20.874265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cd /kaggle/working/RobustXAI\n!ls","metadata":{"execution":{"iopub.status.busy":"2024-10-03T08:55:38.518725Z","iopub.execute_input":"2024-10-03T08:55:38.519104Z","iopub.status.idle":"2024-10-03T08:55:40.574829Z","shell.execute_reply.started":"2024-10-03T08:55:38.519068Z","shell.execute_reply":"2024-10-03T08:55:40.573630Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !python -m experiments.ecg --name feature_importance --train --plot","metadata":{"_uuid":"31d54e82-7762-4bf4-8ada-21f84ace4737","_cell_guid":"ec2b2685-abb3-40cb-86a5-d602e1c769f6","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-10-03T04:34:58.993688Z","iopub.execute_input":"2024-10-03T04:34:58.994615Z","iopub.status.idle":"2024-10-03T04:35:12.277838Z","shell.execute_reply.started":"2024-10-03T04:34:58.994561Z","shell.execute_reply":"2024-10-03T04:35:12.276732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn as nn\nimport os\nimport logging\nimport argparse\nimport pandas as pd\nimport itertools\nfrom pathlib import Path\nfrom torch.utils.data import DataLoader, RandomSampler, Subset\n# from datasets.loaders import ECGDataset\nfrom models.time_series import AllCNN, StandardCNN\nfrom utils.symmetries import Translation1D\nfrom utils.misc import set_random_seed\n# from utils.plots import (\n#     single_robustness_plots,\n#     relaxing_invariance_plots,\n#     enforce_invariance_plot,\n#     sensitivity_plot,\n# )\nfrom interpretability.robustness import (\n    accuracy,\n    InvariantExplainer,\n    model_invariance_exact,\n    explanation_invariance_exact,\n    explanation_equivariance_exact,\n    sensitivity,\n)\nfrom interpretability.example import (\n    SimplEx,\n    RepresentationSimilarity,\n    TracIn,\n    InfluenceFunctions,\n)\nfrom interpretability.feature import FeatureImportance\nfrom interpretability.concept import CAR, CAV, ConceptExplainer\nfrom captum.attr import (\n    IntegratedGradients,\n    GradientShap,\n    FeaturePermutation,\n    FeatureAblation,\n    Occlusion,\n    DeepLift\n)","metadata":{"_uuid":"c793ec2d-4bc5-4b13-8ec3-5852c92ee9fe","_cell_guid":"76956069-b7a9-4fbc-bc65-f5b38b04db49","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-10-03T11:01:10.237175Z","iopub.execute_input":"2024-10-03T11:01:10.237532Z","iopub.status.idle":"2024-10-03T11:01:19.734691Z","shell.execute_reply.started":"2024-10-03T11:01:10.237496Z","shell.execute_reply":"2024-10-03T11:01:19.733890Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import logging\n\nlogger = logging.getLogger()\nlogger.setLevel(logging.INFO)\n\n# 检查是否已有处理器，避免重复添加\n# if not logger.handlers:\nconsole_handler = logging.StreamHandler()\nconsole_handler.setLevel(logging.INFO)\nformatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\nconsole_handler.setFormatter(formatter)\nlogger.addHandler(console_handler)\n\n# 测试日志\nlogger.info(\"This is an info 33message\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-03T11:01:27.133883Z","iopub.execute_input":"2024-10-03T11:01:27.135048Z","iopub.status.idle":"2024-10-03T11:01:27.142166Z","shell.execute_reply.started":"2024-10-03T11:01:27.135003Z","shell.execute_reply":"2024-10-03T11:01:27.141239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### datasets.loaders","metadata":{"_uuid":"57975810-4b2f-4d2b-a8fc-70aef1810513","_cell_guid":"5eff5c4d-4d9a-4cc2-a85e-47ff02834bd2","trusted":true}},{"cell_type":"code","source":"import os\nimport random\nimport re\nfrom abc import ABC, abstractmethod\nfrom collections import Counter\nfrom functools import partial\nfrom pathlib import Path\n\nimport h5py\nimport networkx as nx\nimport nltk\nimport numpy as np\nimport pandas as pd\nimport pytorch_lightning as pl\nimport torch\nimport torch.nn.functional as F\nfrom imblearn.over_sampling import SMOTE\nfrom joblib import Parallel, delayed\nfrom torch.utils.data import Dataset, SubsetRandomSampler\nfrom torch.utils.data.dataset import random_split\nfrom torch_geometric.datasets import TUDataset\nfrom torchvision.datasets import CIFAR100, STL10, FashionMNIST, ImageFolder\nfrom torchvision.transforms import transforms\nfrom tqdm import tqdm\n\nfrom utils.misc import to_molecule\n\nclass ConceptDataset(ABC, Dataset):\n    @property\n    @abstractmethod\n    def concept_names(self):\n        ...\n\n    @abstractmethod\n    def generate_concept_dataset(self, concept_id: int, concept_set_size: int) -> tuple:\n        ...\n\n        \nclass ECGDataset(ConceptDataset):\n    def __init__(\n        self,\n        data_dir: Path,\n        train: bool,\n        balance_dataset: bool,\n        random_seed: int = 42,\n        binarize_label: bool = True,\n    ):\n        \"\"\"\n        Generate a ECG dataset\n        Args:\n            data_dir: directory where the dataset should be stored\n            train: True if the training set should be returned, False for the testing set\n            balance_dataset: True if the classes should be balanced with SMOTE\n            random_seed: random seed for reproducibility\n            binarize_label: True if the label should be binarized (0: normal heartbeat, 1: abnormal heartbeat)\n        \"\"\"\n        self.data_dir = data_dir\n        if not data_dir.exists():\n            os.makedirs(data_dir)\n            self.download()\n        # Read CSV; extract features and labels\n#         file_path = (\n#             data_dir / \"mitbih_train.csv\" if train else data_dir / \"mitbih_test.csv\"\n#         )\n        file_path = (\n            data_dir / \"mitbih_train.csv\" if train else data_dir / \"mitbih_test.csv\"\n        )\n#         data_train = pd.read_csv('../input/heartbeat/mitbih_train.csv', header=None)\n#         data_test = pd.read_csv('../input/heartbeat/mitbih_test.csv', header=None)\n        df = pd.read_csv(file_path)\n        X = df.iloc[:, :187].values\n        y = df.iloc[:, 187].values\n        if balance_dataset:\n            n_normal = np.count_nonzero(y == 0)\n            balancing_dic = {\n                0: n_normal,\n                1: int(n_normal / 4),\n                2: int(n_normal / 4),\n                3: int(n_normal / 4),\n                4: int(n_normal / 4),\n            }\n            smote = SMOTE(random_state=random_seed, sampling_strategy=balancing_dic)\n            X, y = smote.fit_resample(X, y)\n        if binarize_label:\n            y = np.where(y >= 1, 1, 0)\n        self.X = torch.tensor(X, dtype=torch.float32).unsqueeze(1)\n        self.y = torch.tensor(y, dtype=torch.long)\n        self.binarize_label = binarize_label\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, idx):\n        return self.X[idx], self.y[idx]\n\n    def download(self) -> None:\n        import kaggle\n\n        logging.info(f\"Downloading ECG dataset in {self.data_dir}\")\n        kaggle.api.authenticate()\n        kaggle.api.dataset_download_files(\n            \"shayanfazeli/heartbeat\", path=self.data_dir, unzip=True\n        )\n        logging.info(f\"ECG dataset downloaded in {self.data_dir}\")\n\n    def generate_concept_dataset(self, concept_id: int, concept_set_size: int) -> tuple:\n        \"\"\"\n        Return a concept dataset with positive/negatives for ECG\n        Args:\n            random_seed: random seed for reproducibility\n            concept_set_size: size of the positive and negative subset\n        Returns:\n            a concept dataset of the form X (features),C (concept labels)\n        \"\"\"\n        assert not self.binarize_label\n        mask = self.y == concept_id + 1\n        positive_idx = torch.nonzero(mask).flatten()\n        negative_idx = torch.nonzero(~mask).flatten()\n        positive_loader = torch.utils.data.DataLoader(\n            self, batch_size=concept_set_size, sampler=SubsetRandomSampler(positive_idx)\n        )\n        negative_loader = torch.utils.data.DataLoader(\n            self, batch_size=concept_set_size, sampler=SubsetRandomSampler(negative_idx)\n        )\n        X_pos, C_pos = next(iter(positive_loader))\n        X_neg, C_neg = next(iter(negative_loader))\n        X = torch.concatenate((X_pos, X_neg), 0)\n        C = torch.concatenate(\n            (torch.ones(concept_set_size), torch.zeros(concept_set_size)), 0\n        )\n        rand_perm = torch.randperm(len(X))\n        return X[rand_perm], C[rand_perm]\n\n    def concept_names(self):\n        return [\"Supraventricular\", \"Premature Ventricular\", \"Fusion Beats\", \"Unknown\"]","metadata":{"_uuid":"1053662c-500d-4cf4-9aaa-ffdef1ebe3ea","_cell_guid":"490312fd-b9c8-4b6f-a2a2-db0808551b95","execution":{"iopub.status.busy":"2024-10-03T11:01:48.608709Z","iopub.execute_input":"2024-10-03T11:01:48.609405Z","iopub.status.idle":"2024-10-03T11:01:48.636684Z","shell.execute_reply.started":"2024-10-03T11:01:48.609361Z","shell.execute_reply":"2024-10-03T11:01:48.635684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### utils.plots","metadata":{}},{"cell_type":"code","source":"import argparse\nimport itertools\nimport json\nimport logging\nimport textwrap\nfrom pathlib import Path\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport networkx as nx\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\nsns.set_style(\"whitegrid\")\nsns.set_palette(\"colorblind\")\nmarkers = {\n    \"DeepLift\": \"o\",\n    \"Feature Ablation\": \"s\",\n    \"Feature Occlusion\": \"X\",\n    \"Feature Permutation\": \"D\",\n    \"Gradient Shap\": \"v\",\n    \"Integrated Gradients\": \"p\",\n    \"Influence Functions\": \"^\",\n    \"Rep. Similar-Lin1\": \"*\",\n    \"SimplEx-Lin1\": \"H\",\n    \"TracIn\": \">\",\n    \"CAR-Lin1\": \"<\",\n    \"CAV-Lin1\": \"d\",\n}\n\n\ndef single_robustness_plots(plot_dir: Path, dataset: str, experiment_name: str) -> None:\n    metrics_df = pd.read_csv(plot_dir / \"metrics.csv\")\n    for model_type in metrics_df[\"Model Type\"].unique():\n        sub_df = metrics_df[metrics_df[\"Model Type\"] == model_type]\n        y = (\n            \"Explanation Equivariance\"\n            if \"Explanation Equivariance\" in metrics_df.columns\n            else \"Explanation Invariance\"\n        )\n        ax = sns.boxplot(sub_df, x=\"Explanation\", y=y, showfliers=False)\n        wrap_labels(ax, 10)\n        plt.ylim(-1.1, 1.1)\n        plt.tight_layout()\n        plt.savefig(\n            plot_dir\n            / f'{experiment_name}_{dataset}_{model_type.lower().replace(\" \", \"_\")}.pdf'\n        )\n        plt.close()\n\n\ndef global_robustness_plots(experiment_name: str) -> None:\n    sns.set(font_scale=0.9)\n    sns.set_style(\"whitegrid\")\n    sns.set_palette(\"colorblind\")\n    with open(Path.cwd() / \"results_dir.json\") as f:\n        path_dic = json.load(f)\n    global_df = []\n    for dataset in path_dic:\n        dataset_df = pd.read_csv(\n            Path.cwd() / path_dic[dataset] / experiment_name / \"metrics.csv\"\n        )\n        dataset_df[\"Dataset\"] = [dataset] * len(dataset_df)\n        global_df.append(dataset_df)\n    global_df = pd.concat(global_df)\n    rename_dic = {\n        \"SimplEx-Lin1\": \"SimplEx-Inv\",\n        \"SimplEx-Conv3\": \"SimplEx-Equiv\",\n        \"Representation Similarity-Lin1\": \"Rep. Similar-Inv\",\n        \"Representation Similarity-Conv3\": \"Rep. Similar-Equiv\",\n        \"CAR-Lin1\": \"CAR-Inv\",\n        \"CAR-Conv3\": \"CAR-Equiv\",\n        \"CAV-Lin1\": \"CAV-Inv\",\n        \"CAV-Conv3\": \"CAV-Equiv\",\n        \"SimplEx-Phi\": \"SimplEx-Equiv\",\n        \"SimplEx-Rho\": \"SimplEx-Inv\",\n        \"Representation Similarity-Phi\": \"Rep. Similar-Equiv\",\n        \"Representation Similarity-Rho\": \"Rep. Similar-Inv\",\n        \"CAR-Phi\": \"CAR-Equiv\",\n        \"CAR-Rho\": \"CAR-Inv\",\n        \"CAV-Phi\": \"CAV-Equiv\",\n        \"CAV-Rho\": \"CAV-Inv\",\n        \"CAR-Conv1\": \"CAR-Equiv\",\n        \"CAV-Conv1\": \"CAV-Equiv\",\n        \"SimplEx-Conv1\": \"SimplEx-Equiv\",\n        \"Representation Similarity-Conv1\": \"Rep. Similar-Equiv\",\n        \"CAR-Layer3\": \"CAR-Inv\",\n        \"CAV-Layer3\": \"CAV-Inv\",\n        \"SimplEx-Layer3\": \"SimplEx-Inv\",\n        \"Representation Similarity-Layer3\": \"Rep. Similar-Inv\",\n        \"CAR-Embedding\": \"CAR-Inv\",\n        \"CAV-Embedding\": \"CAV-Inv\",\n        \"SimplEx-Embedding\": \"SimplEx-Inv\",\n        \"Representation Similarity-Embedding\": \"Rep. Similar-Inv\",\n    }\n    global_df = global_df.replace(rename_dic)\n    global_df = global_df[\n        (global_df[\"Model Type\"] == \"All-CNN\")\n        | (global_df[\"Model Type\"] == \"GNN\")\n        | (global_df[\"Model Type\"] == \"Deep-Set\")\n        | (global_df[\"Model Type\"] == \"D8-Wide-ResNet\")\n        | (global_df[\"Model Type\"] == \"bow_classifier\")\n    ]\n    y = (\n        \"Explanation Equivariance\"\n        if \"Explanation Equivariance\" in global_df.columns\n        else \"Explanation Invariance\"\n    )\n    ax = sns.boxplot(global_df, x=\"Dataset\", hue=\"Explanation\", y=y, showfliers=False)\n    wrap_labels(ax, 10)\n    plt.ylim(-1.1, 1.1)\n    box_patches = [\n        patch for patch in ax.patches if type(patch) == matplotlib.patches.PathPatch\n    ]\n    if (\n        len(box_patches) == 0\n    ):  # in matplotlib older than 3.5, the boxes are stored in ax2.artists\n        box_patches = ax.artists\n    num_patches = len(box_patches)\n    lines_per_boxplot = len(ax.lines) // num_patches\n    for i, patch in enumerate(box_patches):\n        # Set the linecolor on the patch to the facecolor, and set the facecolor to None\n        col = patch.get_facecolor()\n        patch.set_edgecolor(col)\n        patch.set_facecolor(\"None\")\n\n        # Each box has associated Line2D objects (to make the whiskers, fliers, etc.)\n        # Loop over them here, and use the same color as above\n        for line in ax.lines[i * lines_per_boxplot : (i + 1) * lines_per_boxplot]:\n            line.set_color(col)\n            line.set_mfc(col)  # facecolor of fliers\n            line.set_mec(col)  # edgecolor of fliers\n\n    # Also fix the legend\n    for legpatch in ax.legend_.get_patches():\n        col = legpatch.get_facecolor()\n        legpatch.set_edgecolor(col)\n        legpatch.set_facecolor(\"None\")\n    sns.despine(left=True)\n    plt.tight_layout()\n    plt.savefig(Path.cwd() / f\"results/{experiment_name}_global_robustness.pdf\")\n    plt.close()\n\n\ndef relaxing_invariance_plots(\n    plot_dir: Path, dataset: str, experiment_name: str\n) -> None:\n    sns.set(font_scale=1.2)\n    sns.set_style(\"whitegrid\")\n    sns.set_palette(\"colorblind\")\n    metrics_df = pd.read_csv(plot_dir / \"metrics.csv\")\n    metrics_df = metrics_df.drop(\n        metrics_df[\n            (metrics_df.Explanation == \"SimplEx-Conv3\")\n            | (metrics_df.Explanation == \"Representation Similarity-Conv3\")\n            | (metrics_df.Explanation == \"CAR-Conv3\")\n            | (metrics_df.Explanation == \"CAV-Conv3\")\n        ].index\n    )\n    rename_dic = {\"Representation Similarity-Lin1\": \"Rep. Similar-Lin1\"}\n    metrics_df = metrics_df.replace(rename_dic)\n    y = (\n        \"Explanation Equivariance\"\n        if \"Explanation Equivariance\" in metrics_df.columns\n        else \"Explanation Invariance\"\n    )\n    plot_df = metrics_df.groupby([\"Model Type\", \"Explanation\"]).mean()\n    plot_df[[\"Model Invariance CI\", f\"{y} CI\"]] = (\n        2 * metrics_df.groupby([\"Model Type\", \"Explanation\"]).sem()\n    )\n    unique_explanations = metrics_df[\"Explanation\"].unique()\n    trimmed_markers = {key: value for key, value in markers.items() if key in unique_explanations}\n    sns.scatterplot(\n        plot_df,\n        x=\"Model Invariance\",\n        y=y,\n        hue=\"Model Type\",\n        edgecolor=\"black\",\n        alpha=0.5,\n        style=\"Explanation\",\n        # markers=markers[: metrics_df[\"Explanation\"].unique()],\n        markers=trimmed_markers,\n        s=100,\n    )\n    plt.errorbar(\n        x=plot_df[\"Model Invariance\"],\n        y=plot_df[y],\n        xerr=plot_df[\"Model Invariance CI\"],\n        yerr=plot_df[f\"{y} CI\"],\n        ecolor=\"black\",\n        elinewidth=1.7,\n        linestyle=\"\",\n        capsize=1.7,\n        capthick=1.7,\n    )\n    plt.xscale(\"linear\")\n    plt.axline((0, 0), slope=1, color=\"gray\", linestyle=\"dotted\")\n    plt.xlim(0, 1.1)\n    plt.ylim(0, 1.1)\n    plt.tight_layout()\n    plt.savefig(plot_dir / f\"{experiment_name}_{dataset}_relaxing_invariance.pdf\")\n    plt.close()\n\n\ndef mc_convergence_plot(plot_dir: Path, dataset: str, experiment_name: str) -> None:\n    metrics_df = pd.read_csv(plot_dir / \"metrics.csv\")\n    for estimator_name in metrics_df[\"Estimator Name\"].unique():\n        metrics_subdf = metrics_df[metrics_df[\"Estimator Name\"] == estimator_name]\n        x = metrics_subdf[\"Number of MC Samples\"]\n        y = metrics_subdf[\"Estimator Value\"]\n        ci = 2 * metrics_subdf[\"Estimator SEM\"]\n        plt.plot(x, y, label=estimator_name)\n        plt.fill_between(x, y - ci, y + ci, alpha=0.2)\n    plt.legend()\n    plt.xlabel(r\"$N_{\\mathrm{samp}}$\")\n    plt.ylabel(\"Monte Carlo Estimator\")\n    plt.ylim(-1, 1)\n    plt.tight_layout()\n    plt.savefig(plot_dir / f\"{experiment_name}_{dataset}.pdf\")\n    plt.close()\n\n\ndef understanding_randomness_plots(plot_dir: Path, dataset: str) -> None:\n    data_df = pd.read_csv(plot_dir / \"data.csv\")\n    sub_df = data_df[data_df[\"Baseline\"] == False]\n    print(sub_df)\n    sns.kdeplot(data=data_df, x=\"y1\", y=\"y2\", hue=\"Model Type\", fill=True)\n    for model_type in data_df[\"Model Type\"].unique():\n        baseline = data_df[\n            (data_df[\"Model Type\"] == model_type) & (data_df[\"Baseline\"] == True)\n        ]\n        plt.plot(\n            baseline[\"y1\"],\n            baseline[\"y2\"],\n            marker=\"x\",\n            linewidth=0,\n            label=f\"Baseline {model_type}\",\n        )\n    plt.axhline(0, color=\"black\")\n    plt.axvline(0, color=\"black\")\n    plt.xlabel(r\"$y_1$\")\n    plt.ylabel(r\"$y_2$\")\n    plt.legend()\n    plt.show()\n\n\ndef enforce_invariance_plot(plot_dir: Path, dataset: str) -> None:\n    sns.set(font_scale=1.3)\n    sns.set_style(\"whitegrid\")\n    sns.set_palette(\"colorblind\")\n    metrics_df = pd.read_csv(plot_dir / \"metrics.csv\")\n    sns.lineplot(metrics_df, x=\"N_inv\", y=\"Explanation Invariance\", hue=\"Explanation\")\n    plt.legend()\n    plt.xlabel(r\"$N_{\\mathrm{inv}}$\")\n    plt.tight_layout()\n    plt.savefig(plot_dir / f\"enforce_invariance_{dataset}.pdf\")\n    plt.close()\n\n\ndef sensitivity_plot(plot_dir: Path, dataset: str) -> None:\n    metrics_df = pd.read_csv(plot_dir / \"metrics.csv\")\n    sns.scatterplot(\n        metrics_df,\n        x=\"Explanation Sensitivity\",\n        y=\"Explanation Equivariance\",\n        hue=\"Explanation\",\n        alpha=0.5,\n        s=10,\n    )\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(plot_dir / f\"sensitivity_comparison_{dataset}.pdf\")\n    plt.close()\n\n\ndef draw_molecule(g, edge_mask=None, draw_edge_labels=False):\n    g = g.copy().to_undirected()\n    node_labels = {}\n    for u, data in g.nodes(data=True):\n        node_labels[u] = data[\"name\"]\n    pos = nx.planar_layout(g)\n    pos = nx.spring_layout(g, pos=pos)\n    if edge_mask is None:\n        edge_color = \"black\"\n        widths = None\n    else:\n        edge_color = [edge_mask[(u, v)] for u, v in g.edges()]\n        widths = [x * 10 for x in edge_color]\n    nx.draw(\n        g,\n        pos=pos,\n        labels=node_labels,\n        width=widths,\n        edge_color=edge_color,\n        edge_cmap=plt.cm.Blues,\n        node_color=\"azure\",\n    )\n\n    if draw_edge_labels and edge_mask is not None:\n        edge_labels = {k: (\"%.2f\" % v) for k, v in edge_mask.items()}\n        nx.draw_networkx_edge_labels(g, pos, edge_labels=edge_labels, font_color=\"red\")\n    plt.show()\n\n\ndef wrap_labels(ax, width, break_long_words=False, do_y: bool = False) -> None:\n    \"\"\"\n    Break labels in several lines in a figure\n    Args:\n        ax: figure axes\n        width: maximal number of characters per line\n        break_long_words: if True, allow breaks in the middle of a word\n        do_y: if True, apply the function to the y axis as well\n    Returns:\n    \"\"\"\n    labels = []\n    for label in ax.get_xticklabels():\n        text = label.get_text()\n        labels.append(\n            textwrap.fill(text, width=width, break_long_words=break_long_words)\n        )\n    ax.set_xticklabels(labels, rotation=0)\n    if do_y:\n        labels = []\n        for label in ax.get_yticklabels():\n            text = label.get_text()\n            labels.append(\n                textwrap.fill(text, width=width, break_long_words=break_long_words)\n            )\n        ax.set_yticklabels(labels, rotation=0)\n\n\ndef global_relax_invariance() -> None:\n    sns.set(font_scale=1.2)\n    sns.set_style(\"whitegrid\")\n    with open(Path.cwd() / \"results_dir.json\") as f:\n        path_dic = json.load(f)\n    global_df = []\n    for dataset, experiment_name in itertools.product(\n        [\"ECG\", \"Fa.MNIST\"],\n        [\"feature_importance\", \"example_importance\", \"concept_importance\"],\n    ):\n        dataset_df = pd.read_csv(\n            Path.cwd() / path_dic[dataset] / experiment_name / \"metrics.csv\"\n        )\n        dataset_df[\"Dataset\"] = [dataset] * len(dataset_df)\n        dataset_df[\"Experiment\"] = [experiment_name] * len(dataset_df)\n        dataset_df = dataset_df.drop(\n            dataset_df[\n                (dataset_df.Explanation == \"SimplEx-Conv3\")\n                | (dataset_df.Explanation == \"Representation Similarity-Conv3\")\n                | (dataset_df.Explanation == \"CAR-Conv3\")\n                | (dataset_df.Explanation == \"CAV-Conv3\")\n            ].index\n        )\n        rename_dic = {\"Representation Similarity-Lin1\": \"Rep. Similar-Lin1\"}\n        dataset_df = dataset_df.replace(rename_dic)\n        global_df.append(dataset_df)\n    global_df = pd.concat(global_df)\n\n    n_datasets = len(global_df[\"Dataset\"].unique())\n\n    # Create a grid of plots\n    fig, axs = plt.subplots(nrows=n_datasets, ncols=3, figsize=(17, 9), sharex=True)\n\n    datasets = global_df[\"Dataset\"].unique()\n    y_titles = [\n        \"Feature Importance Equivariance\",\n        \"Example Importance Invariance\",\n        \"Concept Importance Invariance\",\n    ]\n    experiments = global_df[\"Experiment\"].unique()\n    style_handles = []\n    style_labels = []\n    # Loop over the subplots and plot the data\n    for i, dataset in enumerate(datasets):  # rows\n        for j, experiment in enumerate(experiments):  # columns\n            ax = axs[i, j]\n            metrics_df = global_df[\n                (global_df[\"Dataset\"] == dataset)\n                & (global_df[\"Experiment\"] == experiment)\n            ]\n            y = (\n                \"Explanation Equivariance\"\n                if \"feature\" in experiment\n                else \"Explanation Invariance\"\n            )\n            plot_df = metrics_df.groupby([\"Model Type\", \"Explanation\"]).mean(\n                numeric_only=True\n            )\n            plot_df[[\"Model Invariance CI\", f\"{y} CI\"]] = 2 * metrics_df.groupby(\n                [\"Model Type\", \"Explanation\"]\n            )[[\"Model Invariance\", y]].apply(\"sem\")\n            sns.scatterplot(\n                ax=ax,\n                data=plot_df,\n                x=\"Model Invariance\",\n                y=y,\n                hue=\"Model Type\",\n                edgecolor=\"black\",\n                alpha=0.5,\n                style=\"Explanation\",\n                markers=markers,\n                s=200,\n            )\n            ax.errorbar(\n                x=plot_df[\"Model Invariance\"],\n                y=plot_df[y],\n                xerr=plot_df[\"Model Invariance CI\"],\n                yerr=plot_df[f\"{y} CI\"],\n                ecolor=\"black\",\n                elinewidth=1.7,\n                linestyle=\"\",\n                capsize=1.7,\n                capthick=1.7,\n            )\n            ax.set_xscale(\"linear\")\n            ax.axline((0, 0), slope=1, color=\"gray\", linestyle=\"dotted\")\n            ax.set_xlim(0, 1.1)\n            ax.set_ylim(0, 1.1)\n            ax.set_ylabel(y_titles[j])\n            # Get handles and labels for hue and style legends\n            handles, labels = ax.get_legend_handles_labels()\n            explanation_cut = labels.index(\"Explanation\") + int(j > 0)\n            # Create separate legends for hue and style\n            if i == 0 and j == 0:\n                hue_handles = handles[\n                    :explanation_cut\n                ]  # first half of handles are for hue\n                hue_labels = labels[:explanation_cut]\n            if i == len(datasets) - 1:\n                style_handles.extend(handles[explanation_cut:])\n                style_labels.extend(labels[explanation_cut:])\n\n            ax.legend().remove()\n            if j == 1:\n                ax.set_title(dataset)\n    fig.legend(\n        hue_handles + style_handles,\n        hue_labels + style_labels,\n        loc=\"lower center\",\n        ncol=5,\n        bbox_to_anchor=(0.5, -0.1),\n    )\n    # fig.tight_layout()\n\n    plt.savefig(\n        Path.cwd() / f\"results/global_relax_invariance.pdf\", bbox_inches=\"tight\"\n    )\n    plt.close()\n\n\ndef training_dynamic_plot(\n    data_path: Path = Path.cwd() / \"results/d8-wideresnet-training_dynamics.csv\",\n) -> None:\n    sns.set(font_scale=1.0)\n    sns.set_style(\"whitegrid\")\n    df = pd.read_csv(data_path)\n    df = df[\n        [\n            \"epoch\",\n            \"cifar100_d8_wideresnet_seed42 - model_invariance\",\n            \"stl10_d8_wideresnet_seed42 - model_invariance\",\n            \"cifar100_d8_wideresnet_seed42 - gradient_equivariance\",\n            \"stl10_d8_wideresnet_seed42 - gradient_equivariance\",\n        ]\n    ]\n    rename_cols = {\n        \"epoch\": \"Epoch\",\n        \"cifar100_d8_wideresnet_seed42 - model_invariance\": \"CIFAR100 Model Invariance\",\n        \"stl10_d8_wideresnet_seed42 - model_invariance\": \"STL10 Model Invariance\",\n        \"cifar100_d8_wideresnet_seed42 - gradient_equivariance\": \"CIFAR100 Gradient Equivariance\",\n        \"stl10_d8_wideresnet_seed42 - gradient_equivariance\": \"STL10 Gradient Equivariance\",\n    }\n    df = df.rename(columns=rename_cols)\n    data = []\n    for dataset in [\"CIFAR100\", \"STL10\"]:\n        for property in [\"Model Invariance\", \"Gradient Equivariance\"]:\n            for epoch, score in df[[\"Epoch\", f\"{dataset} {property}\"]].values:\n                data.append(\n                    {\n                        \"Dataset\": dataset,\n                        \"Property\": property,\n                        \"Epoch\": epoch,\n                        \"Score\": score,\n                    }\n                )\n\n    plot_df = pd.DataFrame(data)\n    sns.lineplot(data=plot_df, x=\"Epoch\", y=\"Score\", hue=\"Dataset\", style=\"Property\")\n    plt.savefig(Path.cwd() / \"results/training_dynamics.pdf\", bbox_inches=\"tight\")\n\n\n# if __name__ == \"__main__\":\n#     logging.basicConfig(\n#         level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n#     )\n#     parser = argparse.ArgumentParser()\n#     parser.add_argument(\"--experiment_name\", type=str, default=\"feature_importance\")\n#     parser.add_argument(\"--plot_name\", type=str, default=\"relax_invariance\")\n#     parser.add_argument(\"--dataset\", type=str, default=\"ecg\")\n#     parser.add_argument(\"--model\", type=str, default=\"cnn32_seed42\")\n#     parser.add_argument(\"--concept\", type=str, default=None)\n#     args = parser.parse_args()\n#     with open(Path.cwd() / \"results_dir.json\") as f:\n#         path_dic = json.load(f)\n#     dataset_full_names = {\n#         \"ecg\": \"ECG\",\n#         \"mut\": \"Muta.\",\n#         \"mnet\": \"M.Net40\",\n#         \"fashion_mnist\": \"Fa.MNIST\",\n#     }\n#     plot_path = (\n#         (Path.cwd() / path_dic[dataset_full_names[args.dataset]] / args.experiment_name)\n#         if \"global\" not in args.plot_name and args.plot_name != \"training_dynamics\"\n#         else Path.cwd() / \"results\"\n#     )\n\n#     logging.info(f\"Saving {args.plot_name} plot in {str(plot_path)}\")\n#     match args.plot_name:\n#         case \"robustness\":\n#             single_robustness_plots(plot_path, args.dataset, args.experiment_name)\n#         case \"global_robustness\":\n#             global_robustness_plots(args.experiment_name)\n#         case \"relax_invariance\":\n#             relaxing_invariance_plots(plot_path, args.dataset, args.experiment_name)\n#         case \"mc_convergence\":\n#             mc_convergence_plot(plot_path, args.dataset, args.experiment_name)\n#         case \"enforce_invariance\":\n#             enforce_invariance_plot(plot_path, args.dataset)\n#         case \"sensitivity_comparison\":\n#             sensitivity_plot(plot_path, args.dataset)\n#         case \"global_relax_invariance\":\n#             global_relax_invariance()\n#         case \"training_dynamics\":\n#             training_dynamic_plot()\n#         case other:\n#             raise ValueError(\"Unknown plot name\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-03T11:01:56.486543Z","iopub.execute_input":"2024-10-03T11:01:56.486914Z","iopub.status.idle":"2024-10-03T11:01:56.763659Z","shell.execute_reply.started":"2024-10-03T11:01:56.486879Z","shell.execute_reply":"2024-10-03T11:01:56.762887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Path.cwd()","metadata":{"_uuid":"5d6a4442-ca62-4c30-9c6c-536484477901","_cell_guid":"d47c25f5-f561-4ac1-a485-8d08d893335b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-10-03T11:02:00.437506Z","iopub.execute_input":"2024-10-03T11:02:00.438601Z","iopub.status.idle":"2024-10-03T11:02:00.445684Z","shell.execute_reply.started":"2024-10-03T11:02:00.438558Z","shell.execute_reply":"2024-10-03T11:02:00.444769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ecg main","metadata":{}},{"cell_type":"code","source":"\ndef train_ecg_model(\n    random_seed: int,\n    latent_dim: int,\n    batch_size: int,\n    model_name: str = \"model\",\n    model_dir: Path = Path.cwd() / f\"results/ecg/\",\n#     data_dir: Path = Path.cwd() / \"datasets/ecg\",\n    data_dir: Path = Path('../input/heartbeat'),\n) -> None:\n    logging.info(\"Fitting the ECG classifiers\")\n    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n    set_random_seed(random_seed)\n    model_dir = model_dir / model_name\n    if not model_dir.exists():\n        os.makedirs(model_dir)\n    models = {\n        \"All-CNN\": AllCNN(latent_dim, f\"{model_name}_allcnn\"),\n        \"Standard-CNN\": StandardCNN(latent_dim, f\"{model_name}_standard\"),\n        \"Augmented-CNN\": StandardCNN(latent_dim, f\"{model_name}_augmented\"),\n    }\n    train_set = ECGDataset(data_dir, train=True, balance_dataset=True)\n    test_set = ECGDataset(data_dir, train=False, balance_dataset=False)\n    train_loader = DataLoader(train_set, batch_size, shuffle=True)\n    test_loader = DataLoader(test_set, batch_size, shuffle=True)\n    for model_type in models:\n        logging.info(f\"Now fitting a {model_type} classifier\")\n        if model_type == \"Augmented-CNN\":\n            models[model_type].fit(\n                device,\n                train_loader,\n                test_loader,\n                model_dir,\n                augmentation=True,\n                checkpoint_interval=10,\n            )\n        else:\n            models[model_type].fit(\n                device,\n                train_loader,\n                test_loader,\n                model_dir,\n                augmentation=False,\n                checkpoint_interval=10,\n            )\n\ndef feature_importance(\n    random_seed: int,\n    latent_dim: int,\n    batch_size: int,\n    plot: bool,\n    model_name: str = \"model\",\n    model_dir: Path = Path.cwd() / f\"results/ecg/\",\n#   data_dir: Path = Path.cwd() / \"datasets/ecg\",\n    data_dir: Path = Path('../input/heartbeat'),\n    n_test: int = 1000,\n) -> None:\n    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n    set_random_seed(random_seed)\n    test_set = ECGDataset(data_dir, train=False, balance_dataset=False)\n#     print(test_set)\n    test_subset = Subset(test_set, torch.randperm(len(test_set))[:n_test])\n    test_loader = DataLoader(test_subset, batch_size)\n    models = {\n        \"All-CNN\": AllCNN(latent_dim, f\"{model_name}_allcnn\"),\n        \"Standard-CNN\": StandardCNN(latent_dim, f\"{model_name}_standard\"),\n        \"Augmented-CNN\": StandardCNN(latent_dim, f\"{model_name}_augmented\"),\n    }\n    attr_methods = {\n        \"DeepLift\": DeepLift,\n        \"Integrated Gradients\": IntegratedGradients,\n        \"Gradient Shap\": GradientShap,\n        \"Feature Permutation\": FeaturePermutation,\n        \"Feature Ablation\": FeatureAblation,\n        \"Feature Occlusion\": Occlusion,\n    }\n    model_dir = model_dir / model_name\n    save_dir = model_dir / \"feature_importance\"\n    if not save_dir.exists():\n        os.makedirs(save_dir)\n    translation = Translation1D()\n    metrics = []\n    for model_type in models:\n        logging.info(f\"Now working with {model_type} classifier\")\n        model = models[model_type]\n        model.load_metadata(model_dir)\n        model.load_state_dict(torch.load(model_dir / f\"{model.name}.pt\"), strict=False)\n        model.to(device).eval()\n        model_inv = model_invariance_exact(model, translation, test_loader, device)\n        logging.info(f\"Model invariance: {torch.mean(model_inv):.3g}\")\n        for attr_name in attr_methods:\n            logging.info(f\"Now working with {attr_name} explainer\")\n            feat_importance = FeatureImportance(attr_methods[attr_name](model))\n            explanation_equiv = explanation_equivariance_exact(\n                feat_importance, translation, test_loader, device\n            )\n            for inv, equiv in zip(model_inv, explanation_equiv):\n                metrics.append([model_type, attr_name, inv.item(), equiv.item()])\n            logging.info(\n                f\"Explanation equivariance: {torch.mean(explanation_equiv):.3g}\"\n            )\n    metrics_df = pd.DataFrame(\n        data=metrics,\n        columns=[\n            \"Model Type\",\n            \"Explanation\",\n            \"Model Invariance\",\n            \"Explanation Equivariance\",\n        ],\n    )\n    metrics_df.to_csv(save_dir / \"metrics.csv\", index=False)\n    if plot:\n        single_robustness_plots(save_dir, \"ecg\", \"feature_importance\")\n        # save_dir = Path(\"E:/RobustXAI-main/results/ecg/cnn32_seed42/feature_importance\")\n        relaxing_invariance_plots(save_dir, \"ecg\", \"feature_importance\")\n        \ndef example_importance(\n    random_seed: int,\n    latent_dim: int,\n    batch_size: int,\n    plot: bool,\n    model_name: str = \"model\",\n    model_dir: Path = Path.cwd() / f\"results/ecg/\",\n    data_dir: Path = Path.cwd() / \"datasets/ecg\",\n    n_test: int = 1000,\n    n_train: int = 100,\n    recursion_depth: int = 100,\n) -> None:\n    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n    set_random_seed(random_seed)\n    train_set = ECGDataset(data_dir, train=True, balance_dataset=False)\n    train_loader = DataLoader(train_set, n_train, shuffle=True)\n    X_train, Y_train = next(iter(train_loader))\n    X_train, Y_train = X_train.to(device), Y_train.to(device)\n    train_sampler = RandomSampler(\n        train_set, replacement=True, num_samples=recursion_depth * batch_size\n    )\n    train_loader_replacement = DataLoader(train_set, batch_size, sampler=train_sampler)\n    test_set = ECGDataset(data_dir, train=False, balance_dataset=False)\n    test_subset = Subset(test_set, torch.randperm(len(test_set))[:n_test])\n    test_loader = DataLoader(test_subset, batch_size)\n    models = {\n        \"All-CNN\": AllCNN(latent_dim, f\"{model_name}_allcnn\"),\n        \"Standard-CNN\": StandardCNN(latent_dim, f\"{model_name}_standard\"),\n        \"Augmented-CNN\": StandardCNN(latent_dim, f\"{model_name}_augmented\"),\n    }\n    attr_methods = {\n        \"SimplEx\": SimplEx,\n        \"Representation Similarity\": RepresentationSimilarity,\n        \"TracIn\": TracIn,\n        \"Influence Functions\": InfluenceFunctions,\n    }\n    model_dir = model_dir / model_name\n    save_dir = model_dir / \"example_importance\"\n    if not save_dir.exists():\n        os.makedirs(save_dir)\n    translation = Translation1D()\n    metrics = []\n    for model_type in models:\n        logging.info(f\"Now working with {model_type} classifier\")\n        model = models[model_type]\n        model.load_metadata(model_dir)\n        model.load_state_dict(torch.load(model_dir / f\"{model.name}.pt\"), strict=False)\n        model.to(device).eval()\n        model_inv = model_invariance_exact(model, translation, test_loader, device)\n        logging.info(f\"Model invariance: {torch.mean(model_inv):.3g}\")\n        model_layers = {\"Lin1\": model.fc1, \"Conv3\": model.cnn3}\n        for attr_name in attr_methods:\n            logging.info(f\"Now working with {attr_name} explainer\")\n            model.load_state_dict(\n                torch.load(model_dir / f\"{model.name}.pt\"), strict=False\n            )\n            if attr_name in {\"TracIn\", \"Influence Functions\"}:\n                ex_importance = attr_methods[attr_name](\n                    model,\n                    X_train,\n                    Y_train=Y_train,\n                    train_loader=train_loader_replacement,\n                    loss_function=nn.CrossEntropyLoss(),\n                    save_dir=save_dir / model.name,\n                    recursion_depth=recursion_depth,\n                )\n                explanation_inv = explanation_invariance_exact(\n                    ex_importance, translation, test_loader, device\n                )\n                for inv_model, inv_expl in zip(model_inv, explanation_inv):\n                    metrics.append(\n                        [model_type, attr_name, inv_model.item(), inv_expl.item()]\n                    )\n                logging.info(\n                    f\"Explanation invariance: {torch.mean(explanation_inv):.3g}\"\n                )\n            else:\n                for layer_name in model_layers:\n                    ex_importance = attr_methods[attr_name](\n                        model, X_train, Y_train=Y_train, layer=model_layers[layer_name]\n                    )\n                    explanation_inv = explanation_invariance_exact(\n                        ex_importance, translation, test_loader, device\n                    )\n                    ex_importance.remove_hook()\n                    for inv_model, inv_expl in zip(model_inv, explanation_inv):\n                        metrics.append(\n                            [\n                                model_type,\n                                f\"{attr_name}-{layer_name}\",\n                                inv_model.item(),\n                                inv_expl.item(),\n                            ]\n                        )\n                    logging.info(\n                        f\"Explanation invariance for {layer_name}: {torch.mean(explanation_inv):.3g}\"\n                    )\n    metrics_df = pd.DataFrame(\n        data=metrics,\n        columns=[\n            \"Model Type\",\n            \"Explanation\",\n            \"Model Invariance\",\n            \"Explanation Invariance\",\n        ],\n    )\n    metrics_df.to_csv(save_dir / \"metrics.csv\", index=False)\n    if plot:\n        single_robustness_plots(save_dir, \"ecg\", \"example_importance\")\n        relaxing_invariance_plots(save_dir, \"ecg\", \"example_importance\")\n\n\ndef concept_importance(\n    random_seed: int,\n    latent_dim: int,\n    batch_size: int,\n    plot: bool,\n    model_name: str = \"model\",\n    model_dir: Path = Path.cwd() / f\"results/ecg/\",\n    data_dir: Path = Path.cwd() / \"datasets/ecg\",\n    n_test: int = 1000,\n    concept_set_size: int = 100,\n) -> None:\n    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n    set_random_seed(random_seed)\n    train_set = ECGDataset(\n        data_dir, train=True, binarize_label=False, balance_dataset=False\n    )\n    test_set = ECGDataset(\n        data_dir, train=False, balance_dataset=False, binarize_label=False\n    )\n    test_subset = Subset(test_set, torch.randperm(len(test_set))[:n_test])\n    test_loader = DataLoader(test_subset, batch_size)\n    models = {\n        \"All-CNN\": AllCNN(latent_dim, f\"{model_name}_allcnn\"),\n        \"Standard-CNN\": StandardCNN(latent_dim, f\"{model_name}_standard\"),\n        \"Augmented-CNN\": StandardCNN(latent_dim, f\"{model_name}_augmented\"),\n    }\n    attr_methods = {\"CAV\": CAV, \"CAR\": CAR}\n    model_dir = model_dir / model_name\n    save_dir = model_dir / \"concept_importance\"\n    if not save_dir.exists():\n        os.makedirs(save_dir)\n    translation = Translation1D()\n    metrics = []\n    for model_type in models:\n        logging.info(f\"Now working with {model_type} classifier\")\n        model = models[model_type]\n        model.load_metadata(model_dir)\n        model.load_state_dict(torch.load(model_dir / f\"{model.name}.pt\"), strict=False)\n        model.to(device).eval()\n        model_inv = model_invariance_exact(model, translation, test_loader, device)\n        logging.info(f\"Model invariance: {torch.mean(model_inv):.3g}\")\n        model_layers = {\"Lin1\": model.fc1, \"Conv3\": model.cnn3}\n        for layer_name, attr_name in itertools.product(model_layers, attr_methods):\n            logging.info(\n                f\"Now working with {attr_name} explainer on layer {layer_name}\"\n            )\n            conc_importance = attr_methods[attr_name](\n                model, train_set, n_classes=2, layer=model_layers[layer_name]\n            )\n            conc_importance.fit(device, concept_set_size)\n            concept_acc = conc_importance.concept_accuracy(\n                test_set, device, concept_set_size=concept_set_size\n            )\n            for concept_name in concept_acc:\n                logging.info(\n                    f\"Concept {concept_name} accuracy: {concept_acc[concept_name]:.2g}\"\n                )\n            explanation_inv = explanation_invariance_exact(\n                conc_importance, translation, test_loader, device, similarity=accuracy\n            )\n            conc_importance.remove_hook()\n            for inv_model, inv_expl in zip(model_inv, explanation_inv):\n                metrics.append(\n                    [\n                        model_type,\n                        f\"{attr_name}-{layer_name}\",\n                        inv_model.item(),\n                        inv_expl.item(),\n                    ]\n                )\n            logging.info(f\"Explanation invariance: {torch.mean(explanation_inv):.3g}\")\n    metrics_df = pd.DataFrame(\n        data=metrics,\n        columns=[\n            \"Model Type\",\n            \"Explanation\",\n            \"Model Invariance\",\n            \"Explanation Invariance\",\n        ],\n    )\n    metrics_df.to_csv(save_dir / \"metrics.csv\", index=False)\n    if plot:\n        single_robustness_plots(save_dir, \"ecg\", \"concept_importance\")\n        relaxing_invariance_plots(save_dir, \"ecg\", \"concept_importance\")\n\n\ndef enforce_invariance(\n    random_seed: int,\n    latent_dim: int,\n    batch_size: int,\n    plot: bool,\n    model_name: str = \"model\",\n    model_dir: Path = Path.cwd() / f\"results/ecg/\",\n    data_dir: Path = Path.cwd() / \"datasets/ecg\",\n    n_test: int = 1000,\n    concept_set_size: int = 100,\n) -> None:\n    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n    set_random_seed(random_seed)\n    train_set = ECGDataset(\n        data_dir, train=True, binarize_label=False, balance_dataset=False\n    )\n    test_set = ECGDataset(data_dir, train=False, balance_dataset=False)\n    test_subset = Subset(test_set, torch.randperm(len(test_set))[:n_test])\n    test_loader = DataLoader(test_subset, batch_size)\n    models = {\"All-CNN\": AllCNN(latent_dim, f\"{model_name}_allcnn\")}\n    attr_methods = {\"CAV\": CAV, \"CAR\": CAR}\n    model_dir = model_dir / model_name\n    save_dir = model_dir / \"enforce_invariance\"\n    if not save_dir.exists():\n        os.makedirs(save_dir)\n    translation = Translation1D()\n    metrics = []\n    for model_type in models:\n        logging.info(f\"Now working with {model_type} classifier\")\n        model = models[model_type]\n        model.load_metadata(model_dir)\n        model.load_state_dict(torch.load(model_dir / f\"{model.name}.pt\"), strict=False)\n        model.to(device).eval()\n        model_inv = model_invariance_exact(model, translation, test_loader, device)\n        logging.info(f\"Model invariance: {torch.mean(model_inv):.3g}\")\n        for attr_name in attr_methods:\n            logging.info(f\"Now working with {attr_name} explainer\")\n            attr_method = attr_methods[attr_name](\n                model, train_set, n_classes=2, layer=model.cnn3\n            )\n            if isinstance(attr_method, ConceptExplainer):\n                attr_method.fit(device, concept_set_size)\n            for N_inv in [1, 5, 20, 50, 100, 187]:\n                logging.info(\n                    f\"Now working with invariant explainer with N_inv = {N_inv}\"\n                )\n                inv_method = InvariantExplainer(\n                    attr_method,\n                    translation,\n                    N_inv,\n                    isinstance(attr_method, ConceptExplainer),\n                )\n                explanation_inv = explanation_invariance_exact(\n                    inv_method, translation, test_loader, device, similarity=accuracy\n                )\n                logging.info(\n                    f\"N_inv = {N_inv} - Explanation invariance = {torch.mean(explanation_inv):.3g}\"\n                )\n                for inv_expl in explanation_inv:\n                    metrics.append(\n                        [model_type, f\"{attr_name}-Equiv\", N_inv, inv_expl.item()]\n                    )\n    metrics_df = pd.DataFrame(\n        data=metrics,\n        columns=[\"Model Type\", \"Explanation\", \"N_inv\", \"Explanation Invariance\"],\n    )\n    metrics_df.to_csv(save_dir / \"metrics.csv\", index=False)\n    if plot:\n        enforce_invariance_plot(save_dir, \"ecg\")\n\n\ndef sensitivity_comparison(\n    random_seed: int,\n    latent_dim: int,\n    batch_size: int,\n    plot: bool,\n    model_name: str = \"model\",\n    model_dir: Path = Path.cwd() / f\"results/ecg/\",\n    data_dir: Path = Path.cwd() / \"datasets/ecg\",\n    n_test: int = 1000,\n) -> None:\n    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n    set_random_seed(random_seed)\n    test_set = ECGDataset(data_dir, train=False, balance_dataset=False)\n    test_subset = Subset(test_set, torch.randperm(len(test_set))[:n_test])\n    test_loader = DataLoader(test_subset, batch_size)\n    models = {\"Augmented-CNN\": StandardCNN(latent_dim, f\"{model_name}_augmented\")}\n    attr_methods = {\n        \"Integrated Gradients\": IntegratedGradients,\n        \"Gradient Shap\": GradientShap,\n        \"Feature Permutation\": FeaturePermutation,\n        \"Feature Ablation\": FeatureAblation,\n        \"Feature Occlusion\": Occlusion,\n    }\n    model_dir = model_dir / model_name\n    save_dir = model_dir / \"sensitivity\"\n    if not save_dir.exists():\n        os.makedirs(save_dir)\n    translation = Translation1D()\n    metrics = []\n    for model_type in models:\n        logging.info(f\"Now working with {model_type} classifier\")\n        model = models[model_type]\n        model.load_metadata(model_dir)\n        model.load_state_dict(torch.load(model_dir / f\"{model.name}.pt\"), strict=False)\n        model.to(device).eval()\n        for attr_name in attr_methods:\n            logging.info(f\"Now working with {attr_name} explainer\")\n            attr_method = attr_methods[attr_name](model)\n            feat_importance = FeatureImportance(attr_method)\n            explanation_sens = (\n                sensitivity(attr_method, test_loader, device).cpu().numpy()\n            )\n            explanation_equiv = (\n                explanation_equivariance_exact(\n                    feat_importance, translation, test_loader, device\n                )\n                .cpu()\n                .numpy()\n            )\n            corr = np.corrcoef(explanation_sens, explanation_equiv)\n            logging.info(f\"Metrics correlation: {corr[0, 1].item():.3g}\")\n            for sens, equiv in zip(explanation_sens, explanation_equiv):\n                metrics.append([model_type, attr_name, sens, equiv])\n    metrics_df = pd.DataFrame(\n        data=metrics,\n        columns=[\n            \"Model Type\",\n            \"Explanation\",\n            \"Explanation Sensitivity\",\n            \"Explanation Equivariance\",\n        ],\n    )\n    metrics_df.to_csv(save_dir / \"metrics.csv\", index=False)\n    if plot:\n        sensitivity_plot(save_dir, \"ecg\")","metadata":{"_uuid":"e03ae4f2-e220-4d78-9970-6824135bc57d","_cell_guid":"87432825-0d57-430f-93c2-453e9d9093a4","execution":{"iopub.status.busy":"2024-10-03T11:02:07.414641Z","iopub.execute_input":"2024-10-03T11:02:07.415006Z","iopub.status.idle":"2024-10-03T11:02:07.482050Z","shell.execute_reply.started":"2024-10-03T11:02:07.414972Z","shell.execute_reply":"2024-10-03T11:02:07.481091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install networkx","metadata":{"execution":{"iopub.status.busy":"2024-10-03T06:44:21.837919Z","iopub.execute_input":"2024-10-03T06:44:21.838394Z","iopub.status.idle":"2024-10-03T06:44:33.216409Z","shell.execute_reply.started":"2024-10-03T06:44:21.838354Z","shell.execute_reply":"2024-10-03T06:44:33.215185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nimport argparse\n\nlogging.basicConfig(\n        level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n    )\n# sys.argv = sys.argv[:1]\n# parser = argparse.ArgumentParser()\n# parser.add_argument(\"--name\", type=str, default=\"feature_importance\")\n# parser.add_argument(\"--seed\", type=int, default=42)\n# parser.add_argument(\"--batch_size\", type=int, default=500)\n# parser.add_argument(\"--latent_dim\", type=int, default=32)\n# parser.add_argument(\"--train\", action=\"store_true\")\n# parser.add_argument(\"--plot\", action=\"store_true\")\n# parser.add_argument(\"--n_test\", type=int, default=1000)\n# args = parser.parse_args()\n\nclass Args:\n    def __init__(self):\n        self.name = \"feature_importance\"\n        self.seed = 42\n        self.batch_size = 500\n        self.latent_dim = 32\n        self.train = False  # 默认值为 False\n        self.plot = True   # 默认值为 False\n        self.n_test = 1000\n\nargs = Args()\nargs.train = True\n\nprint(args.train)","metadata":{"_uuid":"30a33389-6de4-4b35-b6ef-38a57fa9637b","_cell_guid":"cd1163ac-0331-4759-a5bb-9513e91179a5","execution":{"iopub.status.busy":"2024-10-03T11:02:12.513944Z","iopub.execute_input":"2024-10-03T11:02:12.514700Z","iopub.status.idle":"2024-10-03T11:02:12.521697Z","shell.execute_reply.started":"2024-10-03T11:02:12.514662Z","shell.execute_reply":"2024-10-03T11:02:12.520740Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name = f\"cnn{args.latent_dim}_seed{args.seed}\"\nif args.train:\n    train_ecg_model(\n        args.seed, args.latent_dim, args.batch_size, model_name=model_name\n    )","metadata":{"execution":{"iopub.status.busy":"2024-10-03T11:02:14.263818Z","iopub.execute_input":"2024-10-03T11:02:14.264231Z","iopub.status.idle":"2024-10-03T11:21:10.253816Z","shell.execute_reply.started":"2024-10-03T11:02:14.264189Z","shell.execute_reply":"2024-10-03T11:21:10.252884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### feature_importance","metadata":{}},{"cell_type":"code","source":"print(args.name)","metadata":{"execution":{"iopub.status.busy":"2024-10-03T11:21:10.255906Z","iopub.execute_input":"2024-10-03T11:21:10.256364Z","iopub.status.idle":"2024-10-03T11:21:10.261495Z","shell.execute_reply.started":"2024-10-03T11:21:10.256319Z","shell.execute_reply":"2024-10-03T11:21:10.260627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if args.name == \"feature_importance\":\n    feature_importance(\n        args.seed,\n        args.latent_dim,\n        args.batch_size,\n        args.plot,\n        model_name,\n        n_test=args.n_test,\n    )","metadata":{"_uuid":"6677215d-5b10-4adc-bd65-6ab4cc2dbcd5","_cell_guid":"a9e1028e-fca3-4531-be48-265a43a15365","execution":{"iopub.status.busy":"2024-10-03T11:21:10.262924Z","iopub.execute_input":"2024-10-03T11:21:10.263564Z","iopub.status.idle":"2024-10-03T12:14:55.473876Z","shell.execute_reply.started":"2024-10-03T11:21:10.263516Z","shell.execute_reply":"2024-10-03T12:14:55.473051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### example_importance","metadata":{}},{"cell_type":"code","source":"args.name == \"example_importance\"\nprint(args.name)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"example_importance(\n                args.seed,\n                args.latent_dim,\n                args.batch_size,\n                args.plot,\n                model_name,\n                n_test=args.n_test,\n            )","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### concept_importance","metadata":{}},{"cell_type":"code","source":"args.name == \"concept_importance\"\nconcept_importance(\n        args.seed,\n        args.latent_dim,\n        args.batch_size,\n        args.plot,\n        model_name,\n        n_test=args.n_test,\n    )","metadata":{"execution":{"iopub.status.busy":"2024-10-03T10:23:24.312973Z","iopub.status.idle":"2024-10-03T10:23:24.313467Z","shell.execute_reply.started":"2024-10-03T10:23:24.313200Z","shell.execute_reply":"2024-10-03T10:23:24.313224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### enforce_invariance","metadata":{}},{"cell_type":"code","source":"args.name == \"enforce_invariance\"\nenforce_invariance(\n        args.seed,\n        args.latent_dim,\n        args.batch_size,\n        args.plot,\n        model_name,\n        n_test=args.n_test,\n    )\n","metadata":{"execution":{"iopub.status.busy":"2024-10-03T10:23:24.315167Z","iopub.status.idle":"2024-10-03T10:23:24.315537Z","shell.execute_reply.started":"2024-10-03T10:23:24.315360Z","shell.execute_reply":"2024-10-03T10:23:24.315379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### sensitiity_comparision","metadata":{}},{"cell_type":"code","source":"args.name == \"sensitivity_comparison\":\nsensitivity_comparison(\n        args.seed,\n        args.latent_dim,\n        args.batch_size,\n        args.plot,\n        model_name,\n        n_test=args.n_test,\n    )","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import sys\n# sys.path.append('/kaggle/working')\n# from datasets.ecg.loaders import ECGDataset\n\n# # %% [code] {\"jupyter\":{\"outputs_hidden\":false}}","metadata":{"_uuid":"44f4fa05-a881-466c-bcdd-de8dc184fe01","_cell_guid":"1c4ceb64-59fd-464c-bceb-9eef5d8f7bed","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-10-02T12:46:12.264523Z","iopub.execute_input":"2024-10-02T12:46:12.265046Z","iopub.status.idle":"2024-10-02T12:46:12.339320Z","shell.execute_reply.started":"2024-10-02T12:46:12.264968Z","shell.execute_reply":"2024-10-02T12:46:12.337083Z"},"trusted":true},"execution_count":null,"outputs":[]}]}